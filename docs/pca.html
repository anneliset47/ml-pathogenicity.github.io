<!-- File: pca.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="assets/css/styles.css" />
  <link href='https://cdn.jsdelivr.net/npm/boxicons@2.0.5/css/boxicons.min.css' rel='stylesheet'>
  <title>PCA - Annelise Thorn</title>
</head>
<body>
  <!--===== HEADER NAVIGATION =====-->
  <header class="l-header">
    <nav class="nav bd-grid">
      <div><a href="index.html" class="nav__logo">Annelise Thorn</a></div>
    
      <div class="dropdown">
        <button class="dropbtn">Menu <i class='bx bx-chevron-down'></i></button>
        <div class="dropdown-content">
          <a href="introduction.html">Introduction</a>
          <a href="dataprep_eda.html">Data Prep/EDA</a>
          <a href="clustering.html">Clustering</a>
          <a href="pca.html">PCA</a>
          <a href="naivebayes.html">Naive Bayes</a>
          <a href="dectrees.html">Decision Tree</a>
          <a href="svms.html">SVM</a>
          <a href="nn.html">Neural Net</a>
          <a href="conclusions.html">Conclusion</a>
          <a href="aboutme.html">About Me</a>
          <a href="references.html">References</a>
        </div>
      </div>
    </nav>
  </header>

  <main class="l-main">
    <section class="section" id="pca">
      <h2 class="section-title">Principal Component Analysis (PCA)</h2>
      <div class="pca__container bd-grid">
        <!-- (a) Overview -->
        <h3>Overview</h3>
        <p>
          Principal Component Analysis (PCA) is a linear dimensionality reduction technique that transforms a dataset with possibly correlated variables into a new set of uncorrelated variables known as principal components.
          Each component captures as much variance from the data as possible.
          At the core of PCA are eigenvalues and eigenvectors of the data's covariance matrix. Eigenvectors determine the direction of the new feature space (principal components), while eigenvalues tell us how much variance is explained by each component.
          Dimensionality reduction is crucial when dealing with high-dimensional datasets. It simplifies modeling, improves visualization, and reduces overfitting by retaining only the most informative components. PCA is especially helpful when many features are noisy or correlated.
        </p>

        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; align-items: stretch;">

          <!-- Figure 1 -->
          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/pca.png" alt="PCA Variance Explained" style="height: 250px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 1:</strong> PCA variance distribution. This chart shows how much of the dataset’s total variance is explained by each principal component, helping decide how many components to retain.
            </figcaption>
          </figure>

          <!-- Figure 2 -->
          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/Eigenvalues.png" alt="PCA Projection Concept" style="height: 250px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 2:</strong> PCA concept illustration. A visual example of projecting high-dimensional data onto principal components to simplify structure while preserving relationships.
            </figcaption>
          </figure>

        </div>


        <!-- (b) Data Prep -->
        <h3>Data Preparation</h3>
        <p>
        PCA requires numeric, standardized, unlabeled data. In this project, PCA is applied to the cleaned Ensembl repeat data like repeat length and strand orientation. All features were scaled before PCA was performed.
        </p>
        <p>
        <a href="assets/Datasets/cleaned_ensembl.csv" target="_blank">Download sample PCA dataset</a>
        </p>
        <figure style="text-align: center; max-width: 100%; margin: 2rem auto;">
          <img src="assets/img/cleaned_ensembl_snapshot.png" alt="Ensembl Tandem Repeat Snapshot" style="max-width: 100%; height: 250px;" />
          <figcaption style="font-size: 0.95rem; color: #444; margin-top: 0.5rem;">
            <strong>Figure 3:</strong> Snapshot of the Ensembl tandem repeat dataset. Each row contains information about a repeat region, including chromosome number, start and end positions, repeat type (e.g., trf, dust, TAR1), strand direction, and total repeat length.
          </figcaption>
        </figure>

        <!-- (c) Code -->
        <h3>Code</h3>
        <p>
        The PCA implementation was done in Python using scikit-learn. The full code is available in the GitHub repo linked below.
        </p>
        <p>
        <a href="https://github.com/anneliset47/annelisethorn.github.io/blob/main/Code/PCA.py" target="_blank">View PCA code on GitHub</a>
        </p>

        <!-- (d) Results -->
        <h3>PCA Results</h3>

        <div style="display: flex; justify-content: center; gap: 2rem; flex-wrap: wrap; align-items: stretch;">

          <!-- Figure 4 -->
          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/pca_projection_sampled.png" alt="PCA Projection" style="max-width: auto; height: 300px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 4:</strong> PCA projection of the tandem repeat dataset. Data is transformed onto the first two principal components, revealing structure and separation that supports the presence of meaningful subgroups.
            </figcaption>
          </figure>

          <!-- Figure 5 -->
          <figure style="display: flex; flex-direction: column; align-items: center; max-width: 400px; margin: 0;">
            <img src="assets/img/pca_variance_ratio_sampled.png" alt="Explained Variance by PCA Components" style="max-width: auto; height: 300px;" />
            <figcaption style="font-size: 0.95rem; color: #444; text-align: center; margin-top: 0.5rem;">
              <strong>Figure 5:</strong> Explained variance by principal components. The first two components together capture nearly all of the dataset’s variance, confirming that dimensionality can be reduced without significant information loss.
            </figcaption>
          </figure>

        </div>

        
        <!-- (e) Conclusions -->
        <h3>Conclusions</h3>
        <p>
          PCA successfully reduced the tandem repeat dataset to two main components while preserving nearly all of its original variance. This strong retention suggests that key features, like repeat length and strand orientation, play a major role in explaining the structure of the data. When visualized, the PCA projection showed clear separation between data points, which supports the idea that the dataset is well-organized and a good fit for unsupervised learning methods.
          The way the clusters appeared in PCA space also aligned with the subgroup patterns identified by both K-Means and hierarchical clustering. In this way, PCA not only made the dataset easier to interpret and visualize but also helped highlight important patterns that differentiate different repeat regions. These results reinforce the value of dimensionality reduction in genomic feature analysis.
        </p>
      </div>
    </section>